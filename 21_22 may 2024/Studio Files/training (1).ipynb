{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a834c945-f2cf-43a0-94a9-edf28d6a8c3f",
   "metadata": {},
   "source": [
    "### LLAMA 3 Finetune\n",
    "\n",
    "SFT for following 3 indian languages + english language.\n",
    "\n",
    "1. Hindi\n",
    "2. Telugu\n",
    "3. Tamil\n",
    "4. English\n",
    "\n",
    "We use [unsloth](https://github.com/unslothai/unsloth) library for fine-tuning the `llama3-8b` model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f32e689-beab-4cc9-8864-15f58fac0fe9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbbeaefa-5b83-469f-81bb-9922a1293e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unsloth@ git+https://github.com/unslothai/unsloth.git (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Cloning https://github.com/unslothai/unsloth.git to /tmp/pip-install-nhqnfbgh/unsloth_7ddde97275204d10baebacc6b310a4db\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/unslothai/unsloth.git /tmp/pip-install-nhqnfbgh/unsloth_7ddde97275204d10baebacc6b310a4db\n",
      "\n",
      "  Resolved https://github.com/unslothai/unsloth.git to commit a106e0b256fb9cdb2e423eddd9edef805f292c52\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: tyro in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.8.4)\n",
      "Requirement already satisfied: transformers>=4.38.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.41.0)\n",
      "Requirement already satisfied: datasets>=2.16.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.19.1)\n",
      "Requirement already satisfied: sentencepiece in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.2.0)\n",
      "Requirement already satisfied: tqdm in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.66.4)\n",
      "Requirement already satisfied: psutil in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (5.9.8)\n",
      "Requirement already satisfied: wheel>=0.42.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.43.0)\n",
      "Requirement already satisfied: numpy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.26.4)\n",
      "Requirement already satisfied: protobuf<4.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.20.3)\n",
      "Requirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.14.0)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (16.1.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.3.8)\n",
      "Requirement already satisfied: pandas in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.31.0)\n",
      "Requirement already satisfied: xxhash in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.3.1)\n",
      "Requirement already satisfied: aiohttp in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.9.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.23.1)\n",
      "Requirement already satisfied: packaging in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers>=4.38.2->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.5.15)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers>=4.38.2->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers>=4.38.2->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.4.3)\n",
      "Requirement already satisfied: docstring-parser>=0.14.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.16)\n",
      "Requirement already satisfied: typing-extensions>=4.7.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.11.0)\n",
      "Requirement already satisfied: rich>=11.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (13.7.1)\n",
      "Requirement already satisfied: shtab>=1.5.6 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.7.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.19.0->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.19.0->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.19.0->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.19.0->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.2.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.18.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.16.0)\n",
      "Requirement already satisfied: packaging in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (23.2)\n",
      "Collecting ninja\n",
      "  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting einops\n",
      "  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting flash-attn\n",
      "  Downloading flash_attn-2.5.8.tar.gz (2.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m69.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: xformers in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.0.26.post1)\n",
      "Requirement already satisfied: trl in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.8.6)\n",
      "Requirement already satisfied: peft in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.11.1)\n",
      "Requirement already satisfied: accelerate in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.30.1)\n",
      "Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading einops-0.8.0-py3-none-any.whl (43 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: flash-attn\n",
      "  Building wheel for flash-attn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for flash-attn: filename=flash_attn-2.5.8-cp310-cp310-linux_x86_64.whl size=120853537 sha256=53979129f883680327bf5d13027cd014e2d054f4fb5b8856916686ae315e57d6\n",
      "  Stored in directory: /home/zeus/.cache/pip/wheels/9b/5b/2b/dea8af4e954161c49ef1941938afcd91bb93689371ed12a226\n",
      "Successfully built flash-attn\n",
      "Installing collected packages: ninja, flash-attn, einops\n",
      "Successfully installed einops-0.8.0 flash-attn-2.5.8 ninja-1.11.1.1\n",
      "Requirement already satisfied: xformers in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.0.26.post1)\n",
      "Requirement already satisfied: trl in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.8.6)\n",
      "Requirement already satisfied: peft in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.11.1)\n",
      "Requirement already satisfied: bitsandbytes in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.43.1)\n",
      "Requirement already satisfied: wandb in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install --no-deps packaging ninja einops flash-attn xformers trl peft accelerate \n",
    "!pip install --no-deps xformers trl peft  bitsandbytes wandb -U"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73257035-1a0f-4055-8ef9-3df3dadcc14d",
   "metadata": {},
   "source": [
    "### wandb login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9db0a2b2-c01c-4342-bd51-6a792f75a14f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /teamspace/studios/this_studio/.netrc\n"
     ]
    }
   ],
   "source": [
    "!wandb login ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b6bb43-69ce-4d84-ac8e-9f17e8533843",
   "metadata": {},
   "source": [
    "### HuggingFace login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e5f2385f-959b-4e63-aee3-4d332f0909ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aff1e25c137240ed87fec470dbcd33cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "# This will prompt you to enter your token\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b891c5a3-0f9a-4864-b1d8-40a6fb2b5a76",
   "metadata": {},
   "source": [
    "### Download Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ea0fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fourbit_models = [\n",
    "    \"unsloth/mistral-7b-bnb-4bit\",\n",
    "    \"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\",\n",
    "    \"unsloth/llama-2-7b-bnb-4bit\",\n",
    "    \"unsloth/gemma-7b-bnb-4bit\",\n",
    "    \"unsloth/gemma-7b-it-bnb-4bit\", # Instruct version of Gemma 7b\n",
    "    \"unsloth/gemma-2b-bnb-4bit\",\n",
    "    \"unsloth/gemma-2b-it-bnb-4bit\", # Instruct version of Gemma 2b\n",
    "    \"unsloth/llama-3-8b-bnb-4bit\", # [NEW] 15 Trillion token Llama-3\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "855e6f8c-6faa-4daf-a312-75eadcc525fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Llama patching release 2024.5\n",
      "   \\\\   /|    GPU: NVIDIA A10G. Max memory: 22.191 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.2.1+cu121. CUDA = 8.6. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. Xformers = 0.0.26.post1. FA = True.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = torch.bfloat16 # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use for 4bit quantization\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/llama-3-8b-bnb-4bit\", \n",
    "    # model_name=\"unsloth/gemma-7b-bnb-4bit\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    token = '',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db745722-43de-49e2-9c8f-2eb6d0b18a8e",
   "metadata": {},
   "source": [
    "### LoRA setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9e26eb1e-c9a2-44c6-817f-6d0b9b596e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.5 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    use_gradient_checkpointing = 'unsloth',\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894a305b-46eb-47a7-9cc0-3966714e97e8",
   "metadata": {},
   "source": [
    "### Test tokens for each language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "611b9c50-479b-47ff-b11e-eee7f4790964",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google News is a news aggregator service developed by Google. It presents a continuous flow of links to articles organized from thousands of publishers and magazines. Google News is available as an app on Android, iOS, and the Web. Google released a beta version in September 2002 and the official app in January 2006.\n"
     ]
    }
   ],
   "source": [
    "# English reference text\n",
    "\n",
    "text = \"\"\"Google News is a news aggregator service developed by Google. It presents a continuous flow of links to articles organized from thousands of publishers and magazines. Google News is available as an app on Android, iOS, and the Web. Google released a beta version in September 2002 and the official app in January 2006.\"\"\"\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bcc88604-bec4-4e37-a147-2722e70c2072",
   "metadata": {},
   "outputs": [],
   "source": [
    "hindi_text = \"\"\"Google News Google द्वारा विकसित एक समाचार एग्रीगेटर सेवा है। यह हजारों प्रकाशकों और पत्रिकाओं से व्यवस्थित लेखों के लिंक का निरंतर प्रवाह प्रस्तुत करता है। Google News एंड्रॉइड, iOS और वेब पर एक ऐप के रूप में उपलब्ध है। Google ने सितंबर 2002 में बीटा संस्करण और जनवरी 2006 में आधिकारिक ऐप जारी किया।\"\"\"\n",
    "tamil_text = \"\"\"கூகுள் செய்திகள் என்பது கூகுள் உருவாக்கிய செய்தி சேகரிப்பு சேவையாகும். ஆயிரக்கணக்கான வெளியீட்டாளர்கள் மற்றும் பத்திரிகைகளிலிருந்து ஒழுங்கமைக்கப்பட்ட கட்டுரைகளுக்கான தொடர்ச்சியான இணைப்புகளை இது வழங்குகிறது. Android, iOS மற்றும் இணையத்தில் Google செய்திகள் ஒரு பயன்பாடாகக் கிடைக்கிறது. கூகுள் செப்டம்பர் 2002 இல் பீட்டா பதிப்பையும், ஜனவரி 2006 இல் அதிகாரப்பூர்வ பயன்பாட்டையும் வெளியிட்டது.\"\"\"\n",
    "telugu_text = \"\"\"Google వార్తలు అనేది Google ద్వారా అభివృద్ధి చేయబడిన వార్తా అగ్రిగేటర్ సేవ. ఇది వేలకొద్దీ ప్రచురణకర్తలు మరియు మ్యాగజైన్‌ల నుండి నిర్వహించబడిన కథనాలకు నిరంతర లింక్‌లను అందిస్తుంది. Google వార్తలు Android, iOS మరియు వెబ్‌లో యాప్‌గా అందుబాటులో ఉన్నాయి. గూగుల్ సెప్టెంబరు 2002లో బీటా వెర్షన్‌ను మరియు జనవరి 2006లో అధికారిక యాప్‌ను విడుదల చేసింది.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4a4fa6d1-6c17-4bb3-8c2d-49c0cab8f5af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language:  hindi\n",
      "['Google', '▁News', '▁Google', '▁द्वारा', '▁विक', 'स', 'ित', '▁एक', '▁समा', 'चार', '▁ए', 'ग्री', 'गे']\n",
      "------------------------------------------------------------\n",
      "Language:  tamil\n",
      "['க', 'ூ', 'கு', 'ள்', '▁செய', '்த', 'ிகள்', '▁என்ப', 'து', '▁கூ', 'கு', 'ள்', '▁உ', 'ரு', 'வா', 'க்க', 'ிய', '▁செய', '்த', 'ி', '▁ச']\n",
      "------------------------------------------------------------\n",
      "Language:  telugu\n",
      "['Google', '▁వ', 'ార్', 'త', 'లు', '▁అ', 'నే', 'ది', '▁Google', '▁ద', '్వ', 'ార', 'ా', '▁అ', 'భ', 'ి', 'వ', 'ృ', 'ద్', 'ధ', 'ి', '▁చే', 'య', 'బ', 'డ']\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "language_text = {\n",
    "         'hindi': hindi_text, \n",
    "         'tamil': tamil_text, \n",
    "         'telugu': telugu_text, \n",
    "        }\n",
    "\n",
    "for language in language_text:\n",
    "    print(\"Language: \", language)\n",
    "    print(tokenizer.tokenize(language_text[language][:50]))\n",
    "    print(\"------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5f0c321-75bb-47bf-b592-23475d1898b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets, Dataset\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efc4179-f3da-4c35-8224-e595d2abf627",
   "metadata": {},
   "source": [
    "### Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43c808b3-b649-416a-b7b3-56bd2282f58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_prompt = \"\"\"\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7597f8-2cc3-472a-b3f1-0cb98c39d316",
   "metadata": {},
   "source": [
    "### EOS_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c732ac0c-f15c-4745-8c1a-289b2361dd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1e2919-ea4e-4af8-9877-58e37446f2e7",
   "metadata": {},
   "source": [
    "### Prompts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "066abd09-2e12-44cc-915e-0706250c32ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_prompts_func(examples):\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs       = examples[\"input\"]\n",
    "    outputs      = examples[\"output\"]\n",
    "    texts = []\n",
    "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "        if input is None:\n",
    "            input = \"\"\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "\n",
    "def formatting_prompts_func_telugu(examples):\n",
    "    instructions = examples[\"telugu_instruction\"]\n",
    "    inputs       = examples[\"telugu_input\"]\n",
    "    outputs      = examples[\"telugu_output\"]\n",
    "    texts = []\n",
    "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "        if input is None:\n",
    "            input = \"\"\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "472f531e-043b-4ef0-9dd8-3af38f5324b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(dataset_name, \n",
    "                    split_type, \n",
    "                    processing_func, \n",
    "                    rename_column = False, \n",
    "                    post_processing_steps=False,\n",
    "                    filter_data = False, \n",
    "                    column_name = 'id', \n",
    "                    value = 'alpaca',\n",
    "                    num_samples=20000):\n",
    "    \n",
    "    if isinstance(dataset_name, str):\n",
    "        dataset = load_dataset(dataset_name, split=split_type)\n",
    "    else:\n",
    "        # Assuming dataset_name is a filepath for JSON file\n",
    "        with open(dataset_name, 'r') as file:\n",
    "            data = []\n",
    "            for line_number, line in enumerate(file, 1):\n",
    "                try:\n",
    "                    data.append(json.loads(line))\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"Error parsing JSON at line {line_number}: {e}\")\n",
    "            dataset = Dataset.from_pandas(pd.DataFrame(data))\n",
    "    \n",
    "    if post_processing_steps:\n",
    "        for post_processing_step in post_processing_steps:\n",
    "            dataset = post_processing_step(dataset)\n",
    "    \n",
    "    if rename_column:\n",
    "        dataset = rename(dataset)\n",
    "    \n",
    "    if filter_data:\n",
    "        dataset = filter_dataset(dataset, num_samples, value, column_name)\n",
    "    \n",
    "    dataset = dataset.map(processing_func, batched=True)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# Define the additional processing steps\n",
    "def rename(dataset):\n",
    "    return dataset.rename_column('response', 'output')\n",
    "\n",
    "def filter_dataset(dataset, num_samples, value, column_name):\n",
    "    return dataset.filter(lambda example: value in example[column_name]).shuffle(seed=42).select(range(num_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239607db-365f-4d3c-a10b-583c466f1d54",
   "metadata": {},
   "source": [
    "### Process each language dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b6c6d0-1716-4802-a660-79f9dcd37968",
   "metadata": {},
   "source": [
    "#### English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9d680965-5d5e-40bd-9a67-34d7893dff26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Instruction:\n",
      "Give three tips for staying healthy.\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Response:\n",
      "1. Eat a balanced and nutritious diet: Make sure your meals are inclusive of a variety of fruits and vegetables, lean protein, whole grains, and healthy fats. This helps to provide your body with the essential nutrients to function at its best and can help prevent chronic diseases.\n",
      "\n",
      "2. Engage in regular physical activity: Exercise is crucial for maintaining strong bones, muscles, and cardiovascular health. Aim for at least 150 minutes of moderate aerobic exercise or 75 minutes of vigorous exercise each week.\n",
      "\n",
      "3. Get enough sleep: Getting enough quality sleep is crucial for physical and mental well-being. It helps to regulate mood, improve cognitive function, and supports healthy growth and immune function. Aim for 7-9 hours of sleep each night.<|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "dataset_en = process_dataset(\"yahma/alpaca-cleaned\", \"train\", formatting_prompts_func)\n",
    "\n",
    "print(dataset_en['text'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf9caa7-1ded-415e-970b-5485bfea99c0",
   "metadata": {},
   "source": [
    "#### Hindi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "abbd8aee-9e80-4aa6-bb92-688831f7bd24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Instruction:\n",
      "नीचे एक निर्देश है जो आपको बताता है कि किसी कार्य को कैसे पूरा किया जाए। ऐसा उत्तर लिखें जो अनुरोध को पर्याप्त रूप से पूरा करता हो।\n",
      "\n",
      "### Input:\n",
      "वैश्वीकरण भारत में स्वदेशी संस्कृतियों और भाषाओं के संरक्षण के लिए क्या चुनौती पेश करता है?\n",
      "\n",
      "### Response:\n",
      "वैश्वीकरण भारत में स्वदेशी संस्कृतियों और भाषाओं के संरक्षण के लिए कई चुनौतियों का सामना कर रहा है। एक महत्वपूर्ण चुनौती वैश्विक मीडिया और लोकप्रिय संस्कृति का प्रभाव है, जो अक्सर अंग्रेजी और हिंदी जैसी प्रमुख भाषाओं को बढ़ावा देता है, जिससे स्वदेशी भाषाओं का महत्व कम हो जाता है। इससे स्वदेशी समुदायों के भीतर मूल भाषाओं के अंतर-पीढ़ीगत संचरण में गिरावट आ सकती है।\n",
      "\n",
      "इसके अतिरिक्त, आर्थिक वैश्वीकरण से उपभोक्तावाद और समरूप जीवन शैली का प्रसार हो सकता है, जो पारंपरिक सांस्कृतिक प्रथाओं और मूल्यों को नष्ट कर सकता है। नतीजतन, स्वदेशी समुदायों के भीतर युवा पीढ़ियाँ जीवन जीने के आधुनिक तरीकों को अपनाने के लिए अधिक इच्छुक हो सकती हैं, जिससे वे अपनी विरासत से और दूर हो सकते हैं।\n",
      "\n",
      "इसके अलावा, बेहतर अवसरों की तलाश में शहरी केंद्रों में स्वदेशी आबादी का तेजी से शहरीकरण और पलायन स्वदेशी समुदायों की सामंजस्यपूर्ण प्रकृति को कमजोर कर सकता है, जिससे इन समुदायों के भीतर सांस्कृतिक ज्ञान और भाषा के संचरण पर प्रभाव पड़ सकता है।\n",
      "\n",
      "भारत में स्वदेशी संस्कृतियों और भाषाओं के महत्व को पहचानना और उनके संरक्षण की दिशा में सक्रिय रूप से काम करना महत्वपूर्ण है। देशी भाषाओं में शिक्षा, पारंपरिक ज्ञान का प्रलेखन और स्वदेशी कला और शिल्प को बढ़ावा देने जैसे प्रयास इन समृद्ध और विविध सांस्कृतिक विरासतों की रक्षा करने में महत्वपूर्ण भूमिका निभा सकते हैं।<|end_of_text|>\n",
      "\n",
      "### Instruction:\n",
      "एक चरित्र के बारे में एक मूल कहानी उत्पन्न करें जो एक रहस्यमय लाल दरवाजा पाता है।\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Response:\n",
      "एक बहुत मिलनसार व्यक्ति था जो हर वक्त नयी चीजों को जानना चाहता था। एक दिन गली में से जाते समय उसने एक रहस्यमय लाल दरवाजा देखा। उसे उस मायावी दरवाजे के बारे में पता ही नहीं था, लेकिन उसका उत्सुकता उसे उसे अध्ययन करने के लिए उत्तेजित करने लगी थी।\n",
      "\n",
      "वह दरवाजे के किसी नजदीक नहीं जा सकता था, लेकिन उसके उत्सुकता ने उसे इस परिसर में रहने के लिए मजबूर कर दिया। उसने अपने दोस्तों से ऊपरीत बातें के साथ दरवाजे के बारे में बातचीत की। उनमें से किसी ने उसे बताया कि यह दरवाजा रहस्यमय है और न केवल ताकतवर होने के कारण बंद है, बल्कि लोगों की नैतिकता और अच्छे विचारों के आधार पर भी यह खोलता है।\n",
      "\n",
      "इस प्रेरणादायक बात ने उसे प्रभावित कर दिया। वह इस लाल दरवाजे को खोलने के लिए समझदारी बरत रहा था। यह सफलता से खुश था। द्वार खोलने के बाद वह शोकग्रस्त था क्योंकि दरवाजे के पार एक मंदिर की तस्वीर दी गई थी।\n",
      "\n",
      "उस दिन से, उसे लगा कि वह कम से कम दूसरों को सही मार्ग दिखा सकता है। जब भी वह लोगों के साथ बात करता था, वह उन्हें नैतिक और सही रास्ते पर चलने के लिए प्रेरित करता था। वह अब खुश था कि उसने लाल दरवाजे से नहीं, बल्कि अपनी ऊर्जा से सीखा था।\n",
      "\n",
      "इस कहानी से एक महत्वपूर्ण संदेश है कि हमें हमेशा सच्चे और सही रास्ते पर चलने के लिए तैयार रहना चाहिए। इसके बाद हम दूसरों को भी सही मार्ग दिखा सकते हैं।<|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "dataset_hi1 = process_dataset(\"ravithejads/samvaad-hi-filtered\", \"train\", formatting_prompts_func)\n",
    "\n",
    "dataset_hi2 = process_dataset(\"HydraIndicLM/hindi_alpaca_dolly_67k\", \"train\", formatting_prompts_func, filter_data = True)\n",
    "\n",
    "print(dataset_hi1['text'][0])\n",
    "\n",
    "print(dataset_hi2['text'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2255f4-a16b-4c40-8618-c49423007897",
   "metadata": {},
   "source": [
    "#### Tamil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0364e769-1321-49a7-b6ac-6488c7c339b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Instruction:\n",
      "ஆரோக்கியமாக இருப்பதற்கு மூன்று குறிப்புகளைக் கொடுங்கள்.\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Response:\n",
      "1. சமச்சீரான மற்றும் சத்தான உணவை உண்ணுங்கள்: உங்கள் உணவில் பலவகையான பழங்கள் மற்றும் காய்கறிகள், மெலிந்த புரதம், முழு தானியங்கள் மற்றும் ஆரோக்கியமான கொழுப்புகள் உள்ளன என்பதை உறுதிப்படுத்திக் கொள்ளுங்கள். இது உங்கள் உடலைச் சிறப்பாகச் செயல்பட தேவையான ஊட்டச்சத்துக்களுடன் வழங்க உதவுகிறது மற்றும் நாள்பட்ட நோய்களைத் தடுக்க உதவுகிறது.\n",
      "\n",
      "2. வழக்கமான உடல் செயல்பாடுகளில் ஈடுபடுங்கள்: வலுவான எலும்புகள், தசைகள் மற்றும் இருதய ஆரோக்கியத்தை பராமரிக்க உடற்பயிற்சி முக்கியமானது. ஒவ்வொரு வாரமும் குறைந்தபட்சம் 150 நிமிட மிதமான ஏரோபிக் உடற்பயிற்சி அல்லது 75 நிமிட தீவிரமான உடற்பயிற்சியை இலக்காகக் கொள்ளுங்கள்.\n",
      "\n",
      "3. போதுமான தூக்கத்தைப் பெறுங்கள்: போதுமான தரமான தூக்கம் உடல் மற்றும் மன நலனுக்கு முக்கியமானது. இது மனநிலையை சீராக்க உதவுகிறது, அறிவாற்றல் செயல்பாட்டை மேம்படுத்துகிறது மற்றும் ஆரோக்கியமான வளர்ச்சி மற்றும் நோயெதிர்ப்பு செயல்பாட்டை ஆதரிக்கிறது. ஒவ்வொரு இரவும் 7-9 மணிநேர தூக்கத்தை இலக்காகக் கொள்ளுங்கள்.<|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "print(\"Language: \")\n",
    "\n",
    "dataset_ta = process_dataset(\"abhinand/tamil-alpaca\", \"train\", formatting_prompts_func)\n",
    "\n",
    "print(dataset_ta['text'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed007b33-0d4e-446e-bdd0-b75db4149bc5",
   "metadata": {},
   "source": [
    "#### Telugu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f76df52-d1ab-47be-99ac-c7a181c6518f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Instruction:\n",
      "ఆరోగ్యంగా ఉండటానికి మూడు చిట్కాలు ఇవ్వండి.\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Response:\n",
      "1. సమతుల్య మరియు పోషకమైన ఆహారం తినండి: మీ భోజనంలో వివిధ రకాల పండ్లు మరియు కూరగాయలు, సన్నని ప్రోటీన్, తృణధాన్యాలు మరియు ఆరోగ్యకరమైన కొవ్వులు ఉన్నాయని నిర్ధారించుకోండి. ఇది మీ శరీరానికి ఉత్తమంగా పనిచేయడానికి అవసరమైన పోషకాలను అందించడంలో సహాయపడుతుంది మరియు దీర్ఘకాలిక వ్యాధులను నివారించడంలో సహాయపడుతుంది.\n",
      "\n",
      "2. క్రమం తప్పకుండా శారీరక శ్రమలో పాల్గొనండి: బలమైన ఎముకలు, కండరాలు మరియు హృదయ ఆరోగ్యాన్ని నిర్వహించడానికి వ్యాయామం కీలకం. ప్రతి వారం కనీసం 150 నిమిషాల మితమైన ఏరోబిక్ వ్యాయామం లేదా 75 నిమిషాల తీవ్రమైన వ్యాయామం లక్ష్యంగా పెట్టుకోండి.\n",
      "\n",
      "3. తగినంత నిద్ర పొందండి: శారీరక మరియు మానసిక శ్రేయస్సుకు తగినంత నాణ్యమైన నిద్ర పొందడం చాలా ముఖ్యం. ఇది మానసిక స్థితిని నియంత్రించడానికి, అభిజ్ఞా పనితీరును మెరుగుపరచడానికి మరియు ఆరోగ్యకరమైన పెరుగుదల మరియు రోగనిరోధక పనితీరుకు మద్దతు ఇస్తుంది. ప్రతి రాత్రి 7-9 గంటల నిద్రను లక్ష్యంగా పెట్టుకోండి.<|end_of_text|>\n",
      "\n",
      "### Instruction:\n",
      "మానవ పరిణామ ప్రక్రియ యొక్క అవలోకనాన్ని అందించండి.\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Response:\n",
      "మానవ పరిణామం మిలియన్ల సంవత్సరాల క్రితం మానవులు, చింపాంజీలు మరియు గొరిల్లాల సాధారణ పూర్వీకుల ఆవిర్భావంతో ప్రారంభమైంది. ఈ ప్రక్రియలో వివిధ జాతుల శ్రేణి విడివిడిగా విడిపోయి అభివృద్ధి చెందింది, వీటిలో కొన్ని ఆధునిక మానవులు హోమో సేపియన్స్ కు దారితీశాయి. మానవ పరిణామంలో ముఖ్యమైన మైలురాళ్ళు మొదటి హోమినిడ్లు (మానవ లాంటి ప్రైమేట్లు) కనిపించడం, బైపెడలిజం అభివృద్ధి (నిటారుగా నడవడం) మరియు పెద్ద మెదడుల పెరుగుదల. కాలక్రమేణా, హోమినిడ్లు మరింత అధునాతన సాధనాలు, కమ్యూనికేషన్ సామర్థ్యాలు మరియు సామాజిక నిర్మాణాలను అభివృద్ధి చేశారు. మానవ పరిణామ కాలక్రమంలో ఆర్డిపిథెకస్, ఆస్ట్రలోపిథెకస్, పారాంత్రోపస్ మరియు హోమో వంటి అనేక జాతులు ఉన్నాయి. గుర్తించదగిన జాతులలో అర్డిపిథెకస్ రామిడస్, ఆస్ట్రలోపిథెకస్ అఫారెన్సిస్ 'లూసీ', హోమో హాబిలిస్, హోమో ఎరెక్టస్, హోమో నియాండర్తలెన్సిస్ (నియాండర్తల్స్) మరియు చివరగా హోమో సేపియన్స్ ఉన్నాయి. మానవ పరిణామ ప్రక్రియ వలసలు, వాతావరణ మార్పులు మరియు జన్యు ఉత్పరివర్తనలు వంటి కారకాలచే ప్రభావితమైంది, ఇది నేడు ఉనికిలో ఉన్న వైవిధ్యమైన జనాభా మరియు సంస్కృతులకు దారితీసింది.<|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "dataset_te1 = process_dataset(\"Telugu-LLM-Labs/yahma_alpaca_cleaned_telugu_filtered_and_romanized\", \"train\", formatting_prompts_func_telugu)\n",
    "\n",
    "dataset_te2 = process_dataset(\"Telugu-LLM-Labs/teknium_GPTeacher_general_instruct_telugu_filtered_and_romanized\", \"train\", formatting_prompts_func_telugu)\n",
    "\n",
    "print(dataset_te1['text'][0])\n",
    "\n",
    "print(dataset_te2['text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c24d811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_en.select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ed714a64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['output', 'input', 'instruction', 'text'],\n",
       "    num_rows: 51760\n",
       "})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "535a0a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_en =dataset_en.select(range(1000))\n",
    "dataset_hi1 =dataset_hi1.select(range(1000))\n",
    "dataset_hi2 =dataset_hi2.select(range(1000))\n",
    "dataset_ta =dataset_ta.select(range(1000))\n",
    "dataset_te1 =dataset_te1.select(range(1000))\n",
    "dataset_te2 =dataset_te2.select(range(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89df30d9-f7db-4d1a-8ac6-542adaa44219",
   "metadata": {},
   "source": [
    "### Final dataset concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "71d0c5b1-39a5-417a-a58b-2e5633e95e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = concatenate_datasets([\n",
    "    dataset_en, dataset_hi1, dataset_hi2,\n",
    "     dataset_ta, dataset_te1, dataset_te2\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29007d94-140b-4e7c-890d-7101d09bd527",
   "metadata": {},
   "source": [
    "### Shuffle dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9a91f203-660f-46fc-a47d-42f0ce97ad69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Instruction:\n",
      "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "Previously, the use of dye-sub printing was limited to industrial or high-end commercial printing. Dye-sub photo printing has been used in medical imaging, graphic arts proofing, security, and broadcast-related applications. Today, it is extremely popular in event photography and photo booths or kiosks that require high-speed, on-demand printing.\n",
      "\n",
      "Alps Electric produced the first quality dye-sub printers for home consumers in the $500–$1,000 price range, bringing dye-sublimation technology within the reach of a wider audience. (These models were, however, not true page printers, since they used a narrow printhead that swept across the page, like most inkjet printers.) Now there are many dye-sublimation printers on the market starting from as low as $100, especially postcard-sized mobile photo printers.\n",
      "\n",
      "The ability to produce instant photo prints inexpensively from a small printer has led to dye sublimation solutions supplanting traditional instant photos in some applications, such as ID photography with a card printer.\n",
      "\n",
      "Several corporations market desktop-size units as stand-alone printers and for print kiosk and photo booth applications. Some of these units are based on generic printers. Some manufacturers, offer software development kits with their printers, suggesting that these companies hope to attract system integrators as a potential market.\n",
      "\n",
      "Desktop-size standalone dye-sub photo printers are also used by photographers in event photography. The technology allows photographers to produce and sell lab-quality prints immediately during the event they are attending, with a minimal amount of hardware. \n",
      "\n",
      "Question: Is the Epson F7100 a dye sub printer?\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Response:\n",
      "I'm sorry, but I don't have enough contextual information about the Epson F7100 to answer that question.<|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "print(dataset['text'][100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c6ddfb3c-7dba-439d-9e25-104352937899",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sft = dataset.shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "724b6a80-ac88-4350-82ff-aebb8802f248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Instruction:\n",
      "பின்வரும் பட்டியலை இறங்கு வரிசையில் மறுசீரமைக்கவும்.\n",
      "\n",
      "### Input:\n",
      "9, -7, 15, 12\n",
      "\n",
      "### Response:\n",
      "15, 12, 9, -7<|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "print(dataset_sft['text'][100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "96499bd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['output', 'input', 'instruction', 'text', 'id', 'system_prompt', 'telugu_instruction', 'telugu_input', 'telugu_output', 'telugu_transliterated_instruction', 'telugu_transliterated_input', 'telugu_transliterated_output'],\n",
       "    num_rows: 6000\n",
       "})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1108fa-2d40-4630-96e4-f4a9ce529877",
   "metadata": {},
   "source": [
    "### Count tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2faac191-adc1-4559-8d01-4725501f5e72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(dataset_sft['text'][100])\n",
    "len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd73dd58-814b-48e1-8b6c-4727dd68a5bc",
   "metadata": {},
   "source": [
    "### Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a43e112b-4bac-4a95-a9ab-b2e54947677c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Output directory where the model predictions and checkpoints will be stored\n",
    "# output_dir = \"./results\"\n",
    "\n",
    "# # Number of training epochs\n",
    "# num_train_epochs = 2\n",
    "\n",
    "# # Enable fp16/bf16 training (set bf16 to True with an A100)\n",
    "# fp16 = False\n",
    "# bf16 = True\n",
    "\n",
    "# # Batch size per GPU for training\n",
    "# per_device_train_batch_size = 8\n",
    "\n",
    "# # Batch size per GPU for evaluation\n",
    "# per_device_eval_batch_size = 8\n",
    "\n",
    "# # Number of update steps to accumulate the gradients for\n",
    "# gradient_accumulation_steps = 8\n",
    "\n",
    "# # Enable gradient checkpointing\n",
    "# gradient_checkpointing = True\n",
    "\n",
    "# # Maximum gradient normal (gradient clipping)\n",
    "# max_grad_norm = 1.0\n",
    "\n",
    "# # Initial learning rate (AdamW optimizer)\n",
    "# learning_rate = 2e-4\n",
    "\n",
    "# # Weight decay to apply to all layers except bias/LayerNorm weights\n",
    "# weight_decay = 0.001\n",
    "\n",
    "# # Optimizer to use\n",
    "# optim = \"paged_adamw_32bit\"\n",
    "\n",
    "# # Learning rate schedule\n",
    "# lr_scheduler_type = \"cosine\"\n",
    "\n",
    "# # Number of training steps (overrides num_train_epochs)\n",
    "# max_steps = -1\n",
    "\n",
    "# # Ratio of steps for a linear warmup (from 0 to learning rate)\n",
    "# warmup_ratio = 0.03\n",
    "\n",
    "# # Group sequences into batches with same length\n",
    "# # Saves memory and speeds up training considerably\n",
    "# group_by_length = True\n",
    "\n",
    "# # Save checkpoint every X updates steps\n",
    "# save_steps = 0\n",
    "\n",
    "# # Log every X updates steps\n",
    "# logging_steps = 10\n",
    "\n",
    "# # Pack multiple short examples in the same input sequence to increase efficiency and make training 5x faster for short sequences.\n",
    "# packing = True\n",
    "\n",
    "# # text field in dataset\n",
    "# dataset_text_field = \"text\"\n",
    "\n",
    "# # dataset para,\n",
    "# dataset_num_proc = 2\n",
    "\n",
    "# # Load the entire model on the GPU 0\n",
    "# # device_map = {\"\": 0}\n",
    "# device_map = \"auto\"\n",
    "\n",
    "# # monitoring\n",
    "# report_to = \"wandb\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cb7bce-fd16-4825-be63-9dcec27dbea7",
   "metadata": {},
   "source": [
    "### TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "505ff039-b0f1-412a-a1b8-e840c60655c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 8,\n",
    "        gradient_accumulation_steps = 8,\n",
    "        warmup_steps = 5,\n",
    "        max_steps = 60,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"results\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8aeace4-d76b-4cf6-9e1d-9877ff1c10d2",
   "metadata": {},
   "source": [
    "### Current memory stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f9b9ac44-1811-41f7-8cb1-1443dc5570fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA A10G. Max memory = 22.191 GB.\n",
      "5.594 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3350e847-6572-4d31-9983-8e04a1da8e0a",
   "metadata": {},
   "source": [
    "### Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3c2ec40a-de62-4e81-8a47-133aaf932f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmacharlasaiteja\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/teamspace/studios/this_studio/wandb/run-20240523_031434-o5rzgisr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/macharlasaiteja/llama_3_south/runs/o5rzgisr' target=\"_blank\">LLAMA_3_SOUTH</a></strong> to <a href='https://wandb.ai/macharlasaiteja/llama_3_south' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/macharlasaiteja/llama_3_south' target=\"_blank\">https://wandb.ai/macharlasaiteja/llama_3_south</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/macharlasaiteja/llama_3_south/runs/o5rzgisr' target=\"_blank\">https://wandb.ai/macharlasaiteja/llama_3_south/runs/o5rzgisr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/macharlasaiteja/llama_3_south/runs/o5rzgisr?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f1f93ca06a0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.init(project=\"LLAMA_3_SOUTH\", name=\"LLAMA_3_SOUTH\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041748c4-fe84-4c8d-9827-e6744a1d3b07",
   "metadata": {},
   "source": [
    "### Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e0a1b0cb-b296-4239-9bd9-948f2bcf4a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 6,000 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 8\n",
      "\\        /    Total batch size = 64 | Total steps = 60\n",
      " \"-____-\"     Number of trainable parameters = 41,943,040\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 1:45:53, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.751600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.813600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.686400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.808100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.740800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.733000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.571600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.674700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.723100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.676500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.622600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.639000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.586500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.682900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.632100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.551400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.533700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.591800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.550200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.533600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.529400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.494200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.485000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.586400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.536800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.484800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.532000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.514100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.543200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.549500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.543800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.578600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.501300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.591700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.507300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.553800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.524400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.542500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.556000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.534500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.491500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.565100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.612000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.537800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.519100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.474300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.515500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.489900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.486500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.584100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.553900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.476800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.468400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.517100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.529700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.527100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.494700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.531600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.529600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.558000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615444f4-a5e0-4b7a-845c-3d483111d940",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "886026cc-e132-41d8-8d2e-13b12f0ae3e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57acbd080d3f4566aff10af769cab8fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/574 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75308b84eab244269e31559dd2806bb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/168M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to https://huggingface.co/Saiteja/LLAMA_3_SOUTH\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained(\"LLAMA_3_SOUTH\") # Local saving\n",
    "model.push_to_hub(\"/LLAMA_3_SOUTH\", token = '') \n",
    "tokenizer.push_to_hub(\"/LLAMA_3_SOUTH\", token = '') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f969474-9740-4a2a-a185-3b29dee77a2b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Test Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4f144e6e-9ca4-4ae6-96e3-b9069a6611a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|begin_of_text|>\\n### Instruction:\\n(9+1)+(5+0). इसे 3 चरणों में हल करें.\\n\\n### Input:\\n\\n\\n### Response:\\n1. 9+1 = 10\\n2. 5+0 = 5\\n3. 10+5 = 15<|end_of_text|>']\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        \"(9+1)+(5+0). इसे 3 चरणों में हल करें.\", # instruction\n",
    "        \"\", # input\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens = 300, use_cache = True)\n",
    "print(tokenizer.batch_decode(outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "883dd97f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|begin_of_text|>\\n### Instruction:\\nमुझे खगोल विज्ञान के बारे में बताओ.\\n\\n### Input:\\nprovide Oupput in Telugu\\n\\n### Response:\\nనేను క్షీణమైన గ్రహాన్ని చూస్తాను. ఇది ఒక గ్రహం లేదా సూర్యకుండలో చూస్తాను. నేను వీరిని కలిస్తాను. నేను చూస్తాను. నేను చూస్తాను. నేను చూస్తాను. నేను చూస్తాను. నేను చూస్తాను. నేను చూస్తాను. నేను చూస్తాను. నేను చూస్తాను. నేను చూస్తాను. నేను చ']\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        \"मुझे खगोल विज्ञान के बारे में बताओ.\", # instruction\n",
    "        \"provide Oupput in Telugu\", \n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens = 400, use_cache = True)\n",
    "print(tokenizer.batch_decode(outputs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
